---
title: "Sentiment Analysis with Logistic Regression"
jupyter: "nlp-python-kernel"
format:
#   html:
#     toc: true
#     toc-depth: 3
#     toc-location: left
#     number-sections: true
#     number-depth: 3
#     html-math-method: katex
#     css: styles.css
  gfm:
      html-math-method: katex
      toc: true
      toc-depth: 3
      number-sections: true
      number-depth: 3
---

\newpage

# Supervised ML & Sentiment Analysis

In supervised machine learning, you usually have an input $X$, which goes into your prediction function to get your $\hat{Y}$ You can then compare your prediction with the true value $Y$. This gives you your cost which you use to update the parameters The following image, summarizes the process.

![Diagram representation of Supervised Learning](images/supervised-learning.png)

To perform sentiment analysis on a tweet, you first have to represent the text (i.e. "I am happy because I am learning NLP ") as features, you then train your logistic regression classifier, and then you can use it to classify the text.

![Schematic representation of using supervised learning for sentiment classification.](images/paste-1.png)

Note that in this case, you either classify 1, for a positive sentiment, or 0, for a negative sentiment.

# Text Representation.

## Sparse Representation.

Given a set of tweets:

```{python}
tweets = ['This dog is amazing but I prefer a corgi',
'Donald Trump hair-dresser should be in jail',
'Roses are red, the sky is blue and machine learning is not a black box']
```
A vocabulary $V$ will be formed by all the different words available in the tweet list. Lets imagine that our vocabulary is just composed of the unique words of our first tweet. In this case we could encode the second tweet as follows:

```{python}
sparse_vector = []
vocabulary = tweets[0].split(' ')
for i in tweets[2].split(' '):
    if i in vocabulary:
        sparse_vector.append(1)
    else:
        sparse_vector.append(0)
```

The resulting vector will be $[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0]$ which is a sparse vector as it mostly contain 0s. It is easily seen that this is not scalable as well as not memory efficient. Basically the resulting vectors will have a size of $n=|V|$

## Negative and Positive Frequencies.

Let's imagine that now we have the following tweets:

```{python}
tweets = ['I am happy because I am learning NLP',
'I am happy',
'I am sad, I am not learning NLP',
'I am sad']
```

The unique vocabulary can be obtained by:

```{python}
all_text = ' '.join(tweets).lower()
import string
all_tweets = all_text.translate(str.maketrans('', '', string.punctuation))

# Split the combined string into individual words
words = all_tweets.split()

# Convert the list of words into a set to get unique words
unique_words = set(words)

print(unique_words)

```

For this particular example of sentiment analysis, you have two classes. One class associated with positive sentiment and the other with negative sentiment. So taking your corpus, you'd have a set of two tweets that belong to the positive class, and the sets of two tweets that belong to the negative class. Let's take the sets of positive tweets. Now, take a look at your vocabulary. To get the positive frequency in any word in your vocabulary, you will have to count the times as it appears in the positive tweets and viceversa for the negative frequency.

![](images/negative-positive-dictionary.PNG)

## Feature extraction with Frequencies {#sec-frequencies-1}

Given the representation of the dictionary mentioned above. We can calculate a feature representation vector as follows:


```{python}
positive_negative_dictionary = {
    'vocabulary' : ['I', 'am', 'happy', 'because', 'learning', 'NLP', 'sad', 'not'],
    'posfreq(1)' : [3, 3, 2, 1, 1, 1, 0, 0],
    'negfreq(0)' : [3, 3, 0, 0, 1, 1, 2, 1],
}

sentence = ['I', 'am', 'sad', 'I', 'am', 'not', 'learning', 'NLP']

positions = set(
    [positive_negative_dictionary['vocabulary'].index(word)
    for word in sentence])

pos_freq_sum = sum(
    positive_negative_dictionary['posfreq(1)'][position]
    for position in positions)
neg_freq_sum = sum(
    positive_negative_dictionary['negfreq(0)'][position]
    for position in positions)

X_m = [1, pos_freq_sum, neg_freq_sum]
X_m

```

## Preprocessing techniques.

Lets use now a real dataset from the NLTK library. We will be using the Twitter dataset, the sample dataset from NLTK is separated into positive and negative tweets. It contains 5000 positive tweets and 5000 negative tweets exactly. The exact match between these classes is not a coincidence. The intention is to have a balanced dataset. That does not reflect the real distributions of positive and negative classes in live Twitter streams. It is just because balanced datasets simplify the design of most computational methods that are required for sentiment analysis. However, it is better to be aware that this balance of classes is artificial.

```{python}
import nltk                                # Python library for NLP
from nltk.corpus import twitter_samples    # sample Twitter dataset from NLTK
import matplotlib.pyplot as plt            # library for visualization
import random

# downloads sample twitter dataset.
nltk.download('twitter_samples')
# select the set of positive and negative tweets
all_positive_tweets = twitter_samples.strings('positive_tweets.json')
all_negative_tweets = twitter_samples.strings('negative_tweets.json')

```
We will now get the number of positives and negative tweets:

```{python}

print(f'Number of positive tweets: {len(all_positive_tweets)}')
print(f'Number of negative tweets: {len(all_negative_tweets)}')

print(f'The type of all_positive_tweets is: {type(all_positive_tweets)}')
print(f'The type of a tweet entry is: {type(all_negative_tweets[0])}')
```

Before anything else, we can print a couple of tweets from the dataset to see how they look. Understanding the data is responsible for 80% of the success or failure in data science projects. We can use this time to observe aspects we'd like to consider when preprocessing our data.

Below, you will print one random positive and one random negative tweet. We have added a color mark at the beginning of the string to further distinguish the two. (Warning: This is taken from a public dataset of real tweets and a very small portion has explicit content.)

```{python}

# print positive in greeen
print('\033[92m' + all_positive_tweets[random.randint(0,5000)])

# print negative in red
print('\033[91m' + all_negative_tweets[random.randint(0,5000)])

```

Data preprocessing is one of the critical steps in any machine learning project. It includes cleaning and formatting the data before feeding into a machine learning algorithm. For NLP, the preprocessing steps are comprised of the following tasks:

- Tokenizing the string
- Lowercasing
- Removing stop words and punctuation
- Stemming

All of this techniques allow us to compress even more the dimension of the vocabulary, achieving faster training due to lower feature vector size.

### HyperLink Cleaning

```{python}
import re                                  # library for regular expression operations
import string                              # for string operations

from nltk.corpus import stopwords          # module for stop words that come with NLTK
from nltk.stem import PorterStemmer        # module for stemming
from nltk.tokenize import TweetTokenizer   # module for tokenizing strings
nltk.download('stopwords')

# Our selected sample. Complex enough to exemplify each step
tweet = all_positive_tweets[2277]
print(tweet)
```

Since we have a Twitter dataset, we'd like to remove some substrings commonly used on the platform like the hashtag, retweet marks, and hyperlinks. We'll use the re library to perform regular expression operations on our tweet. We'll define our search pattern and use the sub() method to remove matches by substituting with an empty character (i.e. '')

```{python}
print('\033[92m' + tweet)
print('\033[94m')

# remove old style retweet text "RT"
tweet2 = re.sub(r'^RT[\s]+', '', tweet)

# remove hyperlinks
tweet2 = re.sub(r'https?://[^\s\n\r]+', '', tweet2)

# remove hashtags
# only removing the hash # sign from the word
tweet2 = re.sub(r'#', '', tweet2)

print(tweet2)
```

### Tokenization

```{python}
print()
print('\033[92m' + tweet2)
print('\033[94m')

# instantiate tokenizer class
tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,
                               reduce_len=True)

# tokenize tweets
tweet_tokens = tokenizer.tokenize(tweet2)

print()
print('Tokenized string:')
print(tweet_tokens)
```

### Punctuation and Stopwords

The next step is to remove stop words and punctuation. Stop words are words that don't add significant meaning to the text. You'll see the list provided by NLTK when you run the cells below.

```{python}
#Import the english stop words list from NLTK
#Import the english stop words list from NLTK
stopwords_english = stopwords.words('english')

print('Stop words\n')
print(stopwords_english)

print('\nPunctuation\n')
print(string.punctuation)

```

We can see that the stop words list above contains some words that could be important in some contexts. These could be words like i, not, between, because, won, against. You might need to customize the stop words list for some applications. For our exercise, we will use the entire list.

For the punctuation, we saw earlier that certain groupings like ':)' and '...' should be retained when dealing with tweets because they are used to express emotions. In other contexts, like medical analysis, these should also be removed.

Now we clean the tokenized tweet:

```{python}
print()
print('\033[92m')
print(tweet_tokens)
print('\033[94m')

tweets_clean = []

for word in tweet_tokens: # Go through every word in your tokens list
    if (word not in stopwords_english and  # remove stopwords
        word not in string.punctuation):  # remove punctuation
        tweets_clean.append(word)

print('removed stop words and punctuation:')
print(tweets_clean)
```

### Stemming and lowercasing

Stemming is the process of converting a word to its most general form, or stem. This helps in reducing the size of our vocabulary.

Consider the words:

- learn
- learning
- learned
- learnt

All these words are stemmed from its common root learn. However, in some cases, the stemming process produces words that are not correct spellings of the root word. For example, happi and sunni. That's because it chooses the most common stem for related words. For example, we can look at the set of words that comprises the different forms of happy:

- happy
- happiness
- happier

We can see that the prefix happi is more commonly used. We cannot choose happ because it is the stem of unrelated words like happen.

NLTK has different modules for stemming and we will be using the PorterStemmer module which uses the Porter Stemming Algorithm. Let's see how we can use it in the cell below.

```{python}
print()
print('\033[92m')
print(tweets_clean)
print('\033[94m')

# Instantiate stemming class
stemmer = PorterStemmer()

# Create an empty list to store the stems
tweets_stem = []

for word in tweets_clean:
    stem_word = stemmer.stem(word)  # stemming word
    tweets_stem.append(stem_word)  # append to the list

print('stemmed words:')
print(tweets_stem)
```

### Lemmatization

Another popular library to handle text processing is SpaCy. SpaCy does not have built-in support for stemming but if focuses more in Lemmatization (which is a more sophisticated approach to reduce words to their base form).

Lemmatization (or less commonly lemmatisation) in linguistics is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form.[1]

In computational linguistics, lemmatization is the algorithmic process of determining the lemma of a word based on its intended meaning. Unlike stemming, lemmatization depends on correctly identifying the intended part of speech and meaning of a word in a sentence, as well as within the larger context surrounding that sentence, such as neighbouring sentences or even an entire document. As a result, developing efficient lemmatization algorithms is an open area of research.

An example of using spacy can be found below. First we download the spaCy english model:

```{bash}
pip install spacy
python -m spacy download en_core_web_sm
```

Then we run our pipeline:

```{python}
import spacy
import random
import matplotlib.pyplot as plt

# Load spaCy model
nlp = spacy.load('en_core_web_sm')

# Example tweet
tweet = "RT @username: Example tweet with URL https://t.co/example and #hashtag"

# Preprocessing with spaCy
doc = nlp(tweet)

# Tokenization and processing
# Remove URLs, handles, hashtags, stop words, and punctuation. Then, lemmatize the tokens.
tokens = [
    token.lemma_ for token in doc
    if not token.is_stop
    and not token.is_punct
    and not token.like_url
    and not token.is_space
    and token.text[0] != '#'
    and token.text[0] != '@'
    ]

print("Processed tweet tokens:", tokens)

```

The primary changes with spaCy involve how you handle text processing:

- spaCy's Tokenizer handles tokenization more comprehensively, so you don't need to manually remove RT, URLs, or use regex for hashtags.
- Lemmatization is used instead of stemming. spaCy provides the .lemma_ attribute for tokens.
- Stop word removal is streamlined with token.is_stop.
- Punctuation removal is achieved with token.is_punct.
- spaCy does not require separate download commands for stop words or tokenizers as NLTK does.

## Summing all together

Over all , you start with a given text, you perform preprocessing, then you do feature extraction to convert text into numerical representation as follows:


![](images/basic_feat_engineering_calculation.png)

The feature vector $X$ becomes of dimension $(m,3)$ as follows:

![](images/feature_vector.png)

## Building and Visualizing word frequencies

In our previous example in @sec-frequencies-1 we show how to compute the feature vector given the pre-computed frequencies. Now we will the frequency computation from scratch. To do that we will download the twitter sammples dataset again.

```{python}
import nltk                                  # Python library for NLP
from nltk.corpus import twitter_samples      # sample Twitter dataset from NLTK
import matplotlib.pyplot as plt              # visualization library
import numpy as np                           # library for scientific computing and matrix operations

nltk.download('twitter_samples')
all_positive_tweets = twitter_samples.strings('positive_tweets.json')
all_negative_tweets = twitter_samples.strings('negative_tweets.json')

# concatenate the lists, 1st part is the positive tweets followed by the negative
tweets = all_positive_tweets + all_negative_tweets

# let's see how many tweets we have
print("Number of tweets: ", len(tweets))
labels = np.append(np.ones((len(all_positive_tweets))), np.zeros((len(all_negative_tweets))))

```

To build the frequency computation we will use python dictionaries. In Python, a dictionary is a mutable and indexed collection. It stores items as key-value pairs and uses hash tables underneath to allow practically constant time lookups. In NLP, dictionaries are essential because it enables fast retrieval of items or containment checks even with thousands of entries in the collection.

We will create a function that given a list of tweets it iterates over the whole list, and stores a pair information associated with a given sentiment label.

```{python}

import spacy
nlp = spacy.load('en_core_web_sm')

def process_tweet(tweet):
    """Process tweet function.
    Input:
        tweet: a string containing a tweet
    Output:
        tweets_clean: a list of words containing the processed tweet

    """
    doc = nlp(tweet)

    # Tokenization and processing
    # Remove URLs, handles, hashtags, stop words, and punctuation.
    # Then, lemmatize the tokens.
    tokens = [
        token.lemma_.lower() for token in doc
        if not token.is_stop
        and not token.is_punct
        and not token.like_url
        and not token.is_space
        and token.text[0] != '#'
        and token.text[0] != '@'
        ]

    return tokens

def build_freqs(tweets, ys):
    """Build frequencies.
    Input:
        tweets: a list of tweets
        ys: an m x 1 array with the sentiment label of each tweet
            (either 0 or 1)
    Output:
        freqs: a dictionary mapping each (word, sentiment) pair to its
        frequency
    """
    # Convert np array to list since zip needs an iterable.
    # The squeeze is necessary or the list ends up with one element.
    # Also note that this is just a NOP if ys is already a list.
    yslist = np.squeeze(ys).tolist()

    # Start with an empty dictionary and populate it by looping over all tweets
    # and over all processed words in each tweet.
    freqs = {}
    for y, tweet in zip(yslist, tweets):
        for word in process_tweet(tweet):
            pair = (word, y)
            freqs[pair] = freqs.get(pair, 0) + 1
    return freqs
```

Instead of using the default utils.process_tweet function we will write a new one using spaCy.

We will now create our frequency dictionary:

```{python}
freqs = build_freqs(tweets, labels)

# check data type
print(f'type(freqs) = {type(freqs)}')

# check length of the dictionary
print(f'len(freqs) = {len(freqs)}')

print(freqs)

```

Visualizing our frequency list is definitely not useful so we will try to create a table of word counts:

```{python}
keys = ['happy', 'merry', 'nice', 'good', 'bad', 'sad', 'mad', 'best', 'pretty',
        '❤', ':)', ':(', '😒', '😬', '😄', '😍', '♛',
        'song', 'idea', 'power', 'play', 'magnific']

# list representing our table of word counts.
# each element consist of a sublist with this pattern: [<word>, <positive_count>, <negative_count>]
data = []

# loop through our selected words
for word in keys:

    # initialize positive and negative counts
    pos = 0
    neg = 0

    # retrieve number of positive counts
    if (word, 1) in freqs:
        pos = freqs[(word, 1)]

    # retrieve number of negative counts
    if (word, 0) in freqs:
        neg = freqs[(word, 0)]

    # append the word counts to the table
    data.append([word, pos, neg])

data
```

```{python}
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
sns.set_theme()
fig, ax = plt.subplots(figsize = (8, 8))

# convert positive raw counts to logarithmic scale. we add 1 to avoid log(0)
x = np.log([x[1] + 1 for x in data])

# do the same for the negative counts
y = np.log([x[2] + 1 for x in data])

# Plot a dot for each pair of words
ax.scatter(x, y)

# assign axis labels
plt.xlabel("Log Positive count")
plt.ylabel("Log Negative count")

# Add the word as the label at the same position as you added the points just before
for i in range(0, len(data)):
    ax.annotate(data[i][0], (x[i], y[i]), fontsize=12)

ax.plot([0, 9], [0, 9], color = 'red') # Plot the red line that divides the 2 areas.
plt.show()
```

# Logistic Regression Overview

Logistic regression makes use of the sigmoid function which outputs a probability between 0 and 1. The sigmoid function with some weight parameter
$\theta$ and some input $x^(i)$ is defined as follows:

![](images/log_reg.png)


Note that as $\theta x^(i)$  gets closer and closer to $-\infty$  the denominator of the sigmoid function gets larger and larger and as a result, the sigmoid gets closer to 0. On the other hand, as $\theta x^(i)$ to $+\infty$ the denominator of the sigmoid function gets closer to 1 and as a result the sigmoid also gets closer to
1.

Now given a tweet, you can transform it into a vector and run it through your sigmoid function to get a prediction as follows:

![](images/log_score_for_tweet.png)

## Training logistic regression.

To train your logistic regression function, you will do the following:

![](images/gradient_descent_algo.png)

You initialize your parameter $\theta$, that you can use in your sigmoid, you then compute the gradient that you will use to update
$\theta$, and then calculate the cost. You keep doing so until good enough.

Usually you keep training until the cost converges. The convergence criteria is something that you define upfront as basically a scalar that you consider is good enough as minimum value of the loss.

Lets look to a practical example with the tweets dataset.

```{python}
import numpy as np
import pandas as pd
from nltk.corpus import twitter_samples

from utils import process_tweet, build_freqs

# select the set of positive and negative tweets
all_positive_tweets = twitter_samples.strings('positive_tweets.json')
all_negative_tweets = twitter_samples.strings('negative_tweets.json')

test_pos = all_positive_tweets[4000:]
train_pos = all_positive_tweets[:4000]
test_neg = all_negative_tweets[4000:]
train_neg = all_negative_tweets[:4000]

train_x = train_pos + train_neg
test_x = test_pos + test_neg

# combine positive and negative labels
train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)
test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)

freqs = build_freqs(train_x, train_y)
print("type(freqs) = " + str(type(freqs)))
print("len(freqs) = " + str(len(freqs.keys())))

```


Now we will write a function that computes the extracted features for every tweet and appends it to a feature vector $X$:


```{python}
def extract_features(tweet, freqs, process_tweet=process_tweet):
    '''
    Input:
        tweet: a string containing one tweet
        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)
    Output:
        x: a feature vector of dimension (1,3)
    '''
    # process_tweet tokenizes, lemmas, and removes stopwords
    word_l = process_tweet(tweet)

    # 3 elements in the form of a 1 x 3 vector
    x = np.zeros((1, 3)) #(bias, features, target)

    #bias term is set to 1
    x[0,0] = 1

    # loop through each word in the list of words
    for word in word_l:

            # increment the word count for the positive label 1
            x[0,1] += freqs.get((word, 1.0),0)

            # increment the word count for the negative label 0
            x[0,2] += freqs.get((word, 0.0),0)
    assert(x.shape == (1, 3))
    return x

```

If we test the function with a tweet:


```{python}
# test the function below
print('This is an example of a positive tweet: \n', train_x[0])
print('\nThis is an example of the processed version of the tweet: \n', process_tweet(train_x[0]))
```

And now extracting the features:


```{python}
# Check your function
# test 1
# test on training data
tmp1 = extract_features(train_x[0], freqs)
print(tmp1)
```

We can not create our sigmoid function:

```{python}
def sigmoid(z):
    '''
    Input:
        z: is the input (can be a scalar or an array)
    Output:
        h: the sigmoid of z
    '''
    h = 1/(1+np.exp(-z))
    return h
```

And taking into account the relationship between linear regression and logistic regression we have that:

$z = \theta_{0} x_{0} + \theta_{1} x_{1} + \theta_{2} x_{2} + ... \theta_N x_{N}$

$h(z) = \frac{1}{1+\exp^{-z}}$

The cost function used for logistic regression is the average of the log loss across all training examples:

$$
J(\theta) = -\frac{1}{m} \sum_{i=1}^m y^{(i)}\log (h(z(\theta)^{(i)})) + (1-y^{(i)})\log (1-h(z(\theta)^{(i)}))\tag{1}
$$

Where:

- $m$ is the number of training examples
- $y(i)$ is the actual label of training example 'i'.
- $h(z(i))$ is the model's prediction for the training example 'i'.

To follow the algorithm explained at the beginning of the section we will have to update $\theta$ weights using gradient descent. Calculating the gradient as:

$$
\nabla_{\theta_j}J(\theta) = \frac{1}{m} \sum_{i=1}^m(h^{(i)}-y^{(i)})x^{(i)}_j \tag{2}
$$

Where $i$ is the index across all $m$ training examples, $j$ is the index of weight $\theta_j$, so $x_{j}^{i}$ is the feature associated with $\theta_j$, and the learning rate is $\alpha$ used to update $\theta_j$ as:

$\theta_j = \theta_j - \alpha \times \nabla_{\theta_j}J(\theta)$

Our vanilla algorithm in vector form for updating gradient descent will be:

```{python}
# UNQ_C2 GRADED FUNCTION: gradientDescent
def gradientDescent(x, y, theta, alpha, num_iters):
    '''
    Input:
        x: matrix of features which is (m,n+1)
        y: corresponding labels of the input matrix x, dimensions (m,1)
        theta: weight vector of dimension (n+1,1)
        alpha: learning rate
        num_iters: number of iterations you want to train your model for
    Output:
        J: the final cost
        theta: your final weight vector
    Hint: you might want to print the cost to make sure that it is going down.
    '''
    # get 'm', the number of rows in matrix x
    m = x.shape[0]

    for i in range(0, num_iters):

        # get z, the dot product of x and theta
        z = np.dot(x, theta)

        # get the sigmoid of z
        h = sigmoid(z)

        # calculate the cost function
        J = -1./m * (np.dot(y.T, np.log(h)) + np.dot((1-y).T,np.log(1-h)))

        # update the weights theta
        theta = theta - (alpha/m) * np.dot(x.T,(h-y))

    return J, theta
```

We can now execute a training:


```{python}
# collect the features 'x' and stack them into a matrix 'X'
X = np.zeros((len(train_x), 3))
for i in range(len(train_x)):
    X[i, :]= extract_features(train_x[i], freqs)

# training labels corresponding to X
Y = train_y

# Apply gradient descent
J, theta = gradientDescent(X, Y, np.zeros((3, 1)), 1e-9, 1500)
print(f"The cost after training is {J}.")
print(f"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}")

```
